{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 上面是对PPT内容的总结  \n",
    "> 说一点自己的理解  \n",
    "- Term-document matrices  \n",
    "假设一个 query 被分割为 m 个 term，而有 n 个待查询文档  \n",
    "这个矩阵的大小为 M * N  \n",
    "对于元素 `[i][j]`则表示第`i`个 term 在第`j`个文档的的出现的频率  \n",
    "- bag of words model  词袋模型  \n",
    "不区分query的 term 顺序\n",
    "-  Term frequency tf  \n",
    "$ tf_{t,d} $为term `t` 在文档 `d`中出现的频率  \n",
    "- Log-frequency weighting  \n",
    "$$\n",
    "w_{t,d}=\\begin{cases} \n",
    "1+log_{10}tf_{t,d} & tf_{t,d} > 0 \\\\ \n",
    "0 & \\text otherwise \n",
    "\\end{cases}\n",
    "$$  \n",
    "- document frequency  $ df_{t} $\n",
    "$ df_{t} $ 就是对于 query 的一个 term ，含有这个 term 的 document 数量  \n",
    "- idf  \n",
    "定义 N 为待查询文档的数目  \n",
    "那么\n",
    "$$\n",
    "idf_t=log_{10}(N/df_t)\n",
    "$$  \n",
    "> idf 反映了一个 term 对查询文档的帮助，或者说这个term是否可以作为一个文档的`特性`，如果几乎所有文档都含有此 term，那么这个term就对查询没有什么帮助  \n",
    "\n",
    "- **tf-idf weighting**\n",
    "$$\n",
    "w_{t,d}=log_{10}(1+tf_{t,d})*log_{10}(N/df_t)\n",
    "$$\n",
    "> 进一步，对query来说，对于一个query，我们可以得到任意文档d在此query q上的score  \n",
    "$$\n",
    "Score(q,d)=\\sum_{\\text{t that both in q and d}}tf.idf_{t,d}\n",
    "$$  \n",
    "- 使用向量  \n",
    "这里通俗的讲就是把 query 看作一个 document q，计算这个document q的vector和其他待查寻的document d的相似度，根据相似来排序  \n",
    "排序的方法有欧式距离和余弦相似度，使用**余弦相似度更合理**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 总结一下实现的步骤应该是  \n",
    "- 传入一个查询  \n",
    "- 对查询处理得到m个term  \n",
    "- 计算得到一个 m*n tf-idf矩阵(假设有n个待查询文档)  \n",
    "- 把每一列作为一个document vector $v_i$，对于查询同样得到一个vector $v_o$\n",
    "- 计算 $v_i$ 和 $v_o$的余弦相似度，最后可以得到一个rank  \n",
    "- 输出  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "import re  \n",
    "import math\n",
    "from tqdm import tqdm \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "legal_words=words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=[]\n",
    "all_tweets_id=[]\n",
    "f=open('/home/wangxv/Files/course/message_data/lab1/data/tweets.txt','r')\n",
    "line_num=1\n",
    "for line in f:\n",
    "    tweets.append(json.loads(line))\n",
    "    tweets[-1]['tweetId']=line_num  \n",
    "    line_num+=1\n",
    "    all_tweets_id.append(int(tweets[-1]['tweetId']))\n",
    "f.close()\n",
    "tweets=[{'id':i['tweetId'],'text':i['text']} for i in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对文本进行预处理的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_text(text:str):\n",
    "    text=text.lower()#均转换为小写\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)#仅保留字母和空格  \n",
    "    tokens=word_tokenize(text)#获取分词结果  \n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    filterd_tokens=[word for word in tokens if word not in stop_words]#去除停用词\n",
    "    lemmatizer_tokens=[lemmatizer.lemmatize(word) for word in filterd_tokens]#还原词形\n",
    "    #lemmatizer_tokens=[word for word in lemmatizer_tokens if word in legal_words]#只保留合法单词，加上这个跑得很慢\n",
    "    return lemmatizer_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为tweets添加属性 标记处理后的text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30364/30364 [00:20<00:00, 1499.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "for i in tqdm(range(len(tweets))):\n",
    "    words=deal_text(tweets[i]['text'])\n",
    "    word_dict=defaultdict(lambda: 0)\n",
    "    for word in words:\n",
    "        word_dict[word]+=1  \n",
    "    tweets[i]['words']=word_dict  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建一个单词-文档频率字典，加速之后的idf的计算(要不每次都要遍历30000多个文档，太慢了)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30364/30364 [00:16<00:00, 1844.68it/s]\n",
      "100%|██████████| 53240/53240 [17:47<00:00, 49.85it/s]\n"
     ]
    }
   ],
   "source": [
    "help_dict=defaultdict(lambda:0)\n",
    "words=[]\n",
    "#根据文档构建一个单词列表  \n",
    "tweet_words=[]\n",
    "for tweet in tqdm(tweets):\n",
    "    words+=deal_text(tweet['text'])\n",
    "    tweet_words.append(deal_text(tweet['text']))\n",
    "words=list(set(words))\n",
    "for word in tqdm(words):\n",
    "    for tmp in tweet_words:\n",
    "        if(word in tmp):\n",
    "            help_dict[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存到文本文件\n",
    "with open('help_dict.txt', 'w') as f:\n",
    "    for key, value in help_dict.items():\n",
    "        f.write(f\"{key}:{value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "help_dict = defaultdict(int)\n",
    "with open('help_dict.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        key, value = line.strip().split(':')\n",
    "        help_dict[key] = int(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算 term-frequency  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf(term,document):#传入term，document  \n",
    "    return math.log10(1+document['words'][term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf(term,documents):\n",
    "    f=help_dict[term]  \n",
    "    f=1 if f==0 else f  \n",
    "    return math.log10(len(documents)/f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(term,document,documents=tweets):\n",
    "    return get_tf(term,document)*get_idf(term,documents)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义根据查询获取排序的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(v1,v2):\n",
    "    up=0\n",
    "    for ind in range(len(v1)):\n",
    "        up+=v1[ind]*v2[ind] \n",
    "    down=1\n",
    "    tmp=0  \n",
    "    for e in v1:\n",
    "        tmp+=e**2  \n",
    "    down*=math.sqrt(tmp)\n",
    "    tmp=0\n",
    "    for e in v2:\n",
    "        tmp+=e**2  \n",
    "    down*=math.sqrt(tmp)\n",
    "    if(down==0):\n",
    "        return 0\n",
    "    return math.fabs(up/down)\n",
    "def retrieve(query,documents=tweets):\n",
    "    terms=deal_text(query)\n",
    "    document_vectors=[[0 for i in range(len(terms))] for u in range(len(documents))]\n",
    "    for ind1 in tqdm(range(len(terms))):\n",
    "        for ind2 in range(len(documents)):\n",
    "            document_vectors[ind2][ind1]=get_weight(terms[ind1],documents[ind2])\n",
    "    q_vector=[1 for i in terms]\n",
    "    #计算角度   \n",
    "    result=[(ind+1,cos(q_vector,document_vectors[ind])) for ind in range(len(document_vectors))]\n",
    "    return sorted(result,key=lambda x:x[1],reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='machine learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=retrieve(query)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
