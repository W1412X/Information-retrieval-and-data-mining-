{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 上面是对PPT内容的总结  \n",
    "> 说一点自己的理解  \n",
    "- Term-document matrices  \n",
    "假设一个 query 被分割为 m 个 term，而有 n 个待查询文档  \n",
    "这个矩阵的大小为 M * N  \n",
    "对于元素 `[i][j]`则表示第`i`个 term 在第`j`个文档的的出现的频率  \n",
    "- bag of words model  词袋模型  \n",
    "不区分query的 term 顺序\n",
    "-  Term frequency tf  \n",
    "$ tf_{t,d} $为term `t` 在文档 `d`中出现的频率  \n",
    "- Log-frequency weighting  \n",
    "$$\n",
    "w_{t,d}=\\begin{cases} \n",
    "1+log_{10}tf_{t,d} & tf_{t,d} > 0 \\\\ \n",
    "0 & \\text otherwise \n",
    "\\end{cases}\n",
    "$$  \n",
    "- document frequency  $ df_{t} $\n",
    "$ df_{t} $ 就是对于 query 的一个 term ，含有这个 term 的 document 数量  \n",
    "- idf  \n",
    "定义 N 为待查询文档的数目  \n",
    "那么\n",
    "$$\n",
    "idf_t=log_{10}(N/df_t)\n",
    "$$  \n",
    "> idf 反映了一个 term 对查询文档的帮助，或者说这个term是否可以作为一个文档的`特性`，如果几乎所有文档都含有此 term，那么这个term就对查询没有什么帮助  \n",
    "\n",
    "- **tf-idf weighting**\n",
    "$$\n",
    "w_{t,d}=log_{10}(1+tf_{t,d})*log_{10}(N/df_t)\n",
    "$$\n",
    "> 进一步，对query来说，对于一个query，我们可以得到任意文档d在此query q上的score  \n",
    "$$\n",
    "Score(q,d)=\\sum_{\\text{t that both in q and d}}tf.idf_{t,d}\n",
    "$$  \n",
    "- 使用向量  \n",
    "这里通俗的讲就是把 query 看作一个 document q，计算这个document q的vector和其他待查寻的document d的相似度，根据相似来排序  \n",
    "排序的方法有欧式距离和余弦相似度，使用**余弦相似度更合理**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 总结一下实现的步骤应该是  \n",
    "- 传入一个查询  \n",
    "- 对查询处理得到m个term  \n",
    "- 计算得到一个 m*n tf-idf矩阵(假设有n个待查询文档)  \n",
    "- 把每一列作为一个document vector $v_i$，对于查询同样得到一个vector $v_o$\n",
    "- 计算 $v_i$ 和 $v_o$的余弦相似度，最后可以得到一个rank  \n",
    "- 输出  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "import re  \n",
    "import math\n",
    "from tqdm import tqdm \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "legal_words=words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=[]\n",
    "all_tweets_id=[]\n",
    "f=open('./data/tweets.txt','r')\n",
    "line_num=1\n",
    "for line in f:\n",
    "    tweets.append(json.loads(line))\n",
    "    tweets[-1]['tweetId']=line_num  \n",
    "    line_num+=1\n",
    "    all_tweets_id.append(int(tweets[-1]['tweetId']))\n",
    "f.close()\n",
    "tweets=[{'id':i['tweetId'],'text':i['text']} for i in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对文本进行预处理的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_text(text:str):\n",
    "    text=text.lower()#均转换为小写\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)#仅保留字母和空格  \n",
    "    tokens=word_tokenize(text)#获取分词结果  \n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    filterd_tokens=[word for word in tokens if word not in stop_words]#去除停用词\n",
    "    lemmatizer_tokens=[lemmatizer.lemmatize(word) for word in filterd_tokens]#还原词形\n",
    "    #lemmatizer_tokens=[word for word in lemmatizer_tokens if word in legal_words]#只保留合法单词，加上这个跑得很慢\n",
    "    return lemmatizer_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为tweets添加属性 标记处理后的text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30364 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30364/30364 [00:11<00:00, 2697.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "#构建词汇表，方便之后的向量生成\n",
    "global_words = set()\n",
    "for i in tqdm(range(len(tweets))):\n",
    "    words=deal_text(tweets[i]['text'])\n",
    "    tweets[i]['length']=len(words)#添加文档长度信息\n",
    "    global_words.update(words)#顺便构建全局词汇表\n",
    "    word_dict=defaultdict(lambda: 0)\n",
    "    for word in words:\n",
    "        word_dict[word]+=1  \n",
    "    tweets[i]['words']=word_dict  \n",
    "global_words = list(global_words)  #转为列表方便索引\n",
    "word_index = {word: i for i, word in enumerate(global_words)} #词到索引的映射"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建一个单词-文档频率字典，加速之后的idf的计算(要不每次都要遍历30000多个文档，太慢了)  \n",
    "- 这里只需要运行一次，有了文件之后直接运行下一个就可获取help_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help_dict=defaultdict(lambda:0)\n",
    "words=[]\n",
    "#根据文档构建一个单词列表  \n",
    "tweet_words=[]\n",
    "for tweet in tqdm(tweets):\n",
    "    words+=deal_text(tweet['text'])\n",
    "    tweet_words.append(deal_text(tweet['text']))\n",
    "words=list(set(words))\n",
    "for word in tqdm(words):\n",
    "    for tmp in tweet_words:\n",
    "        if(word in tmp):\n",
    "            help_dict[word]+=1\n",
    "#保存到文本文件\n",
    "with open('help_dict.txt', 'w') as f:\n",
    "    for key, value in help_dict.items():\n",
    "        f.write(f\"{key}:{value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "help_dict = defaultdict(int)\n",
    "with open('help_dict.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        key, value = line.strip().split(':')\n",
    "        help_dict[key] = int(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算 term-frequency  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf(term,document):#传入term，document  \n",
    "    return math.log10(1+document['words'][term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf(term,documents):\n",
    "    f=help_dict[term]  \n",
    "    f=1 if f==0 else f  \n",
    "    return math.log10(len(documents)/f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(term,document,documents=tweets):#在这里添加长度惩罚\n",
    "    return get_tf(term,document)*get_idf(term,documents)/document['length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义根据查询获取排序的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(v1,v2):\n",
    "    up=0\n",
    "    for ind in range(len(v1)):\n",
    "        up+=v1[ind]*v2[ind] \n",
    "    down=1\n",
    "    tmp=0  \n",
    "    for e in v1:\n",
    "        tmp+=e**2  \n",
    "    down*=math.sqrt(tmp)\n",
    "    tmp=0\n",
    "    for e in v2:\n",
    "        tmp+=e**2  \n",
    "    down*=math.sqrt(tmp)\n",
    "    if(down==0):\n",
    "        return 0\n",
    "    return math.fabs(up/down)\n",
    "def if_contain_key(words_key,words_dict):\n",
    "    for key in words_key:\n",
    "        if(words_dict[key]!=0):\n",
    "            return True  \n",
    "    return False \n",
    "# 检索函数\n",
    "def retrieve(query, documents=tweets):\n",
    "    #处理查询并构建查询向量\n",
    "    terms = deal_text(query)\n",
    "    query_vector = [0] * len(global_words)\n",
    "    for term in tqdm(terms):\n",
    "        if term in word_index:\n",
    "            query_vector[word_index[term]] = 1 #这里设置为1，课上讲的简化\n",
    "    #构建所有文档的向量，好吧，构架不了，这里文档数量*词汇量把我内存搞爆了，所以选择在这里直接找含有那个查询关键词的文档构建应该可以吧？\n",
    "    #因为没有包含关键词的文档向量都是0了，没有区分度，排序也欸有意义\n",
    "    selected_documents=[i for i in documents if if_contain_key(terms,i['words'])]\n",
    "    document_vectors = [[0] * len(global_words) for _ in tqdm(range(len(selected_documents)))]\n",
    "    for doc_id, document in tqdm(enumerate(selected_documents)):\n",
    "        for term in document['words']:\n",
    "            if term in word_index:\n",
    "                document_vectors[doc_id][word_index[term]] = get_weight(term, document)\n",
    "    #计算每个文档与查询的相似度\n",
    "    results = [\n",
    "        (selected_documents[ind]['id'], cos(query_vector, document_vectors[ind]))\n",
    "        for ind in range(len(selected_documents))\n",
    "    ]\n",
    "    return sorted(results, key=lambda x: x[1], reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 16578.28it/s]\n",
      "100%|██████████| 49/49 [00:00<00:00, 5309.66it/s]\n",
      "49it [00:00, 35077.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14902, 0.3379670540586379), (21503, 0.3264219283049397), (11257, 0.32232043134124416), (8416, 0.2749069700044688), (10992, 0.25602730412713154), (11288, 0.25412019167289196), (16603, 0.24576169010143106), (20215, 0.22941864832251613), (10648, 0.22536474647753965), (21435, 0.22324902030905638), (7397, 0.2209199661143098), (21991, 0.22004434050789773), (22005, 0.22004434050789773), (19609, 0.2150140872589951), (14973, 0.2146896348146268), (25760, 0.21211922167297903), (26040, 0.21211922167297903), (12052, 0.2118336622131355), (18696, 0.2112100574555666), (14288, 0.20994675618927755), (18517, 0.2098245403576975), (19038, 0.2094459689215885), (13517, 0.20907245946559092), (4271, 0.20817471708192126), (14750, 0.2076566312222904), (1962, 0.20593412384729715), (20956, 0.19714376830663594), (10529, 0.19688443845749468), (6696, 0.19471997887678816), (4250, 0.1937347757522228), (25442, 0.19263657646184373), (1371, 0.19238362705870585), (26275, 0.1920891124160952), (5375, 0.19154877471355714), (11391, 0.1914593681361676), (13527, 0.18870428911823708), (13624, 0.186745590214615), (13962, 0.186745590214615), (29042, 0.1801389756529235), (14046, 0.17973057424047229), (1069, 0.17872751829513994), (8415, 0.17791400499469165), (3477, 0.17710608874478623), (10320, 0.17463518924237745), (15170, 0.17358417828446704), (10593, 0.17148155798268136), (20149, 0.17111722924097464), (17406, 0.16672227069588694), (9642, 0.16025327021312796)]\n",
      "^.^ my neighbor had a snow blower machine and did the whole sidewalk :)\n"
     ]
    }
   ],
   "source": [
    "query='machine learning'\n",
    "result=retrieve(query)\n",
    "print(result)\n",
    "#然后这里对应输出的id和实际documents的索引是差了一个1(输出的是id是行号)\n",
    "print(tweets[result[0][0]-1]['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
